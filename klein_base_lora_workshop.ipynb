{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a66709eb",
   "metadata": {},
   "source": [
    "# Mechanics of LoRA Fine‑Tuning for **FLUX.2 [klein] Base** (Non‑Distilled)\n",
    "## What Actually Matters\n",
    "\n",
    "\n",
    "FLUX.2 [klein] **Base** (non‑distilled, flow matching, 50 steps) — not the distilled Klein models.\n",
    "\n",
    "---\n",
    "\n",
    "**Session map (45 min)**\n",
    "1. Why Klein Base behaves differently\n",
    "2. Where LoRA goes and what it learns\n",
    "3. Hyperparameters that actually move the needle\n",
    "4. Data strategy + failure diagnosis \n",
    "5. Practical recipe + Q&A \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0a60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display helper for local SVGs\n",
    "from pathlib import Path\n",
    "from IPython.display import SVG, display\n",
    "\n",
    "ASSETS = Path('assets')\n",
    "\n",
    "print('Ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2f10a",
   "metadata": {},
   "source": [
    "---\n",
    "# 1) Why **Klein** behaves differently\n",
    "\n",
    "A undistilled model made for Customization and LoRA. \n",
    "FLUX.2 [klein] learns concepts quickly, those notes are based on my personal experience with FLUX models and a bit with FLUX.2 [klein] \n",
    "\n",
    "LoRA feels *more stable* on Klein Base, but it can still overfit quickly on small datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f52e53d-fd0b-4b91-ab6c-80c59f1c2e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter‑efficiency reminder (keep light)\n",
    "display(SVG(ASSETS / 'lora_decomposition.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa6eb4",
   "metadata": {},
   "source": [
    "---\n",
    "# 2) Where LoRA goes (and why it matters)\n",
    "\n",
    "Klein has **double‑stream** blocks followed by **single‑stream** blocks. This yields a useful targeting map:\n",
    "\n",
    "- **Double‑stream (0–4)**: text‑image binding (trigger ↔ concept).\n",
    "- **Single early (5–10)**: texture, color, low‑level style.\n",
    "- **Single mid (11–18)**: parts, identity, structure.\n",
    "- **Single late (19–24)**: composition & layout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture overview\n",
    "display(SVG(ASSETS / 'klein_architecture.svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4c572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer‑wise intuition + targeting options\n",
    "display(SVG(ASSETS / 'layer_analysis.svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d00a2",
   "metadata": {},
   "source": [
    "---\n",
    "# 3) Hyperparameters that actually matter\n",
    "\n",
    "Four knobs drive most outcomes:\n",
    "- **Rank** (capacity)\n",
    "- **Alpha** (strength scaling)\n",
    "- **Learning rate** (speed)\n",
    "- **Steps** (duration)\n",
    "\n",
    "**Recommended starting points (Klein Base 4B)**\n",
    "- **Style**: rank 16, alpha 16, lr 3e‑4, 500–800 steps\n",
    "- **Character / Face**: rank 32, alpha 32, lr 2e‑4, 800–1200 steps\n",
    "- **Object / Product**: rank 24, alpha 24, lr 2e‑4, 600–1000 steps\n",
    "- **I2I Transform**: rank 32, alpha 32, lr 1e‑4, 250–500 steps\n",
    "\n",
    "**Key insight**: `alpha/rank` ≈ effective strength. Keeping it ≈1.0 is a safe default.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d0028",
   "metadata": {},
   "source": [
    "## 3.1 Rank vs Alpha\n",
    "\n",
    "What *changes* when you touch them\n",
    "\n",
    "**Rank = capacity** (how much detail the LoRA can store).\n",
    "**Alpha = strength** (how loud that stored change is).\n",
    "\n",
    "**Guidelines**\n",
    "- Keep **alpha ≈ rank** to start.\n",
    "- If concept is *weak* → increase **rank** before increasing **alpha**.\n",
    "- If concept is *too strong / bleeds* → reduce **alpha** or steps.\n",
    "\n",
    "Two LoRAs with the same `alpha/rank` ratio can still behave differently. Higher rank usually *generalizes* better but can overfit faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b71f2",
   "metadata": {},
   "source": [
    "## 3.2 Learning Rate × Steps\n",
    "\n",
    "**Rule of thumb**\n",
    "- Higher LR → fewer steps, faster learning, higher instability risk.\n",
    "- Lower LR → more steps, stable but can underfit.\n",
    "\n",
    "**Start here (Klein Base 4B)**\n",
    "- LR **1e‑4 to 3e‑4**\n",
    "- Steps **500–1200**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b19205",
   "metadata": {},
   "source": [
    "## 3.2.1 Concept strength vs prompt flexibility (intuition)\n",
    "\n",
    "- As steps increase, **concept strength** rises.\n",
    "- After a point, **prompt flexibility** drops (overfitting).\n",
    "\n",
    "The goal is to stop at the *peak of concept strength* **before** flexibility collapses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2167e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strength vs Flexibility tradeoff\n",
    "display(SVG(ASSETS / 'strength_flexibility_tradeoff.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0hqc48m6t0l",
   "metadata": {},
   "source": [
    "---\n",
    "## 3.3 Comparison: Rank and Steps\n",
    "\n",
    "**Setup**\n",
    "- Same dataset (filmlut style transfer)\n",
    "- Same trigger word (`filmlut`)\n",
    "- Varied: **rank** (8, 16, 32) and **steps** (250, 500, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eftuoxh5gwk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Load samples from training runs\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "SAMPLES_DIR = Path(\"/Users/stephen/Documents/bfl/flux_workshops/assets/all_samples\")\n",
    "\n",
    "def get_final_sample(lora_name, prompt_idx=0):\n",
    "    \"\"\"Get the final (highest step) sample for a config and prompt.\"\"\"\n",
    "    prompt_dirs = {\n",
    "        0: \"prompt0_woman\",\n",
    "        1: \"prompt1_mountains\",\n",
    "        2: \"prompt2_man\",\n",
    "        3: \"prompt3_other\"\n",
    "    }\n",
    "    prompt_dir = SAMPLES_DIR / prompt_dirs.get(prompt_idx, \"prompt0_woman\")\n",
    "    \n",
    "    if not prompt_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Find all samples for this config\n",
    "    pattern = f\"{lora_name}_*_{prompt_idx}.jpg\"\n",
    "    samples = list(prompt_dir.glob(pattern))\n",
    "    \n",
    "    if not samples:\n",
    "        return None\n",
    "    \n",
    "    # Sort by step number and return the last one\n",
    "    def get_step(path):\n",
    "        match = re.search(r'__(\\d+)_\\d+\\.jpg$', path.name)\n",
    "        return int(match.group(1)) if match else 0\n",
    "    \n",
    "    samples.sort(key=get_step)\n",
    "    return Image.open(samples[-1])\n",
    "\n",
    "print(f\"Sample directories: {list(SAMPLES_DIR.iterdir())}\")\n",
    "print(\"Ready to display comparisons!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rzubdsjlo1a",
   "metadata": {},
   "source": [
    "### Rank Comparison (8 → 16 → 32)\n",
    "\n",
    "All trained for **500 steps**. Same prompt.\n",
    "\n",
    "**Typical pattern**\n",
    "- Style strength usually increases with rank.\n",
    "- Diminishing returns often appear after rank 32.\n",
    "- Watch for artifacts at higher capacity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mae4pckeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different ranks at 500 steps (woman portrait)\n",
    "ranks = [8, 16, 32]\n",
    "images_by_rank = {}\n",
    "\n",
    "for rank in ranks:\n",
    "    lora_name = f\"filmlut_r{rank}_s500\"\n",
    "    img = get_final_sample(lora_name, prompt_idx=0)\n",
    "    if img:\n",
    "        images_by_rank[rank] = img\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, len(images_by_rank), figsize=(5*len(images_by_rank), 5))\n",
    "if len(images_by_rank) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, rank in enumerate(images_by_rank.keys()):\n",
    "    axes[i].imshow(images_by_rank[rank])\n",
    "    axes[i].set_title(f\"Rank {rank}\\n(500 steps)\", fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Effect of Rank on Style Transfer (Woman Portrait)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q56nfb2te6i",
   "metadata": {},
   "source": [
    "### Steps Comparison (250 → 500 → 1000)\n",
    "\n",
    "All at **rank 16**.\n",
    "\n",
    "**Typical pattern**\n",
    "- 250 steps: concept begins to appear.\n",
    "- 500 steps: clearer concept with better flexibility.\n",
    "- 1000 steps: stronger concept, higher overfit risk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "so6zrz4godh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different training steps at rank 16\n",
    "steps_list = [250, 500, 1000]\n",
    "images_by_steps = {}\n",
    "\n",
    "for steps in steps_list:\n",
    "    lora_name = f\"filmlut_r16_s{steps}\"\n",
    "    img = get_final_sample(lora_name, prompt_idx=0)\n",
    "    if img:\n",
    "        images_by_steps[steps] = img\n",
    "\n",
    "# Display comparison\n",
    "fig, axes = plt.subplots(1, len(images_by_steps), figsize=(5*len(images_by_steps), 5))\n",
    "if len(images_by_steps) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, steps in enumerate(images_by_steps.keys()):\n",
    "    axes[i].imshow(images_by_steps[steps])\n",
    "    axes[i].set_title(f\"{steps} steps\\n(rank 16)\", fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Effect of Training Steps (Woman Portrait)\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5rthaqeuzyh",
   "metadata": {},
   "source": [
    "### Generalization Test: Does it work on different subjects?\n",
    "\n",
    "A good LoRA applies the style across **different subjects**, not just the training images.\n",
    "\n",
    "Test prompts used:\n",
    "- **Prompt 0**: Woman portrait (cinematic)\n",
    "- **Prompt 1**: Mountain landscape\n",
    "- **Prompt 2**: Man portrait\n",
    "- **Prompt 3**: Other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xe7p269q5nl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generalization test: same LoRA, different prompts\n",
    "lora_name = \"filmlut_r16_s500\"  # Our \"sweet spot\" config\n",
    "\n",
    "prompt_labels = [\"Woman Portrait\", \"Landscape\", \"Man Portrait\"]\n",
    "images = []\n",
    "\n",
    "for prompt_idx in range(3):\n",
    "    img = get_final_sample(lora_name, prompt_idx=prompt_idx)\n",
    "    if img:\n",
    "        images.append((prompt_labels[prompt_idx], img))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(images), figsize=(5*len(images), 5))\n",
    "if len(images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, (label, img) in enumerate(images):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(label, fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Same LoRA ({lora_name}) - Different Subjects\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zf5i2a7cxlc",
   "metadata": {},
   "source": [
    "### Key Takeaways from the Comparisons\n",
    "\n",
    "**Rank**\n",
    "- Higher rank = more capacity to learn detail\n",
    "- Rank 8–16 is often sufficient for style transfer\n",
    "- Diminishing returns after rank 32\n",
    "\n",
    "**Generalization**\n",
    "- Style should transfer across portraits and landscapes\n",
    "- This indicates the LoRA learned the *style*, not memorized images\n",
    "\n",
    "**Stop training** when samples look good **and** the style transfers to new subjects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qlel11gcez",
   "metadata": {},
   "source": [
    "### Multi-Subject Grid: Same LoRA settings, different prompts\n",
    "\n",
    "The goal is consistent style across **different subjects**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cqsrb4n7x96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-subject grid: rows = prompts, columns = configs\n",
    "configs = [\"filmlut_r8_s500\", \"filmlut_r16_s500\", \"filmlut_r32_s500\"]\n",
    "prompt_labels = [\"Woman\", \"Landscape\", \"Man\"]\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 12))\n",
    "\n",
    "for i, prompt_idx in enumerate(range(3)):\n",
    "    for j, config in enumerate(configs):\n",
    "        img = get_final_sample(config, prompt_idx=prompt_idx)\n",
    "        if img:\n",
    "            axes[i, j].imshow(img)\n",
    "        axes[i, j].axis('off')\n",
    "        \n",
    "        if i == 0:\n",
    "            rank = config.split('_')[1]  # e.g., \"r8\"\n",
    "            axes[i, j].set_title(f\"Rank {rank[1:]}\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Row labels\n",
    "for i, label in enumerate(prompt_labels):\n",
    "    axes[i, 0].annotate(label, xy=(-0.15, 0.5), xycoords='axes fraction',\n",
    "                        fontsize=12, fontweight='bold', ha='right', va='center')\n",
    "\n",
    "plt.suptitle(\"Style Transfer Across Subjects (500 steps)\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nThe filmlut cinematic style should be visible in ALL cells.\")\n",
    "print(\"This proves generalization - the LoRA learned the STYLE, not specific images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t4ts7hhjp6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full grid comparison: rank x steps (woman portrait)\n",
    "ranks = [8, 16, 32]\n",
    "steps_list = [250, 500, 1000]\n",
    "\n",
    "grid_images = {}\n",
    "for rank in ranks:\n",
    "    for steps in steps_list:\n",
    "        lora_name = f\"filmlut_r{rank}_s{steps}\"\n",
    "        grid_images[(rank, steps)] = get_final_sample(lora_name, prompt_idx=0)\n",
    "\n",
    "# Display as grid\n",
    "fig, axes = plt.subplots(len(ranks), len(steps_list), figsize=(12, 12))\n",
    "\n",
    "for i, rank in enumerate(ranks):\n",
    "    for j, steps in enumerate(steps_list):\n",
    "        img = grid_images.get((rank, steps))\n",
    "        if img:\n",
    "            axes[i, j].imshow(img)\n",
    "        else:\n",
    "            axes[i, j].text(0.5, 0.5, \"N/A\", ha='center', va='center', fontsize=20)\n",
    "        axes[i, j].axis('off')\n",
    "        \n",
    "        if i == 0:\n",
    "            axes[i, j].set_title(f\"{steps} steps\", fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add row labels\n",
    "for i, rank in enumerate(ranks):\n",
    "    axes[i, 0].annotate(f\"Rank {rank}\", xy=(-0.15, 0.5), xycoords='axes fraction',\n",
    "                        fontsize=12, fontweight='bold', ha='right', va='center')\n",
    "\n",
    "plt.suptitle(\"LoRA Hyperparameter Grid: Rank × Steps\", fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nReading the grid:\")\n",
    "print(\"→ Left to right: more training (250 → 500 → 1000 steps)\")\n",
    "print(\"↓ Top to bottom: more capacity (rank 8 → 16 → 32)\")\n",
    "print(\"\\nSweet spot for this task: rank 16, 500 steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c073a11f-ab69-4bba-8732-0c83df23c5cc",
   "metadata": {},
   "source": [
    "## 8.2 Weight Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaff4ef-f50f-4f8b-9868-205ea7353f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(SVG(ASSETS / 'merge_weights.svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77ae39c",
   "metadata": {},
   "source": [
    "## Dataset size by task (Klein Base)\n",
    "\n",
    "- **Style**: 50–200 images\n",
    "- **Character / face**: 20–60 images (highly curated)\n",
    "- **Object / product**: 30–100 images\n",
    "- **I2I transformation**: 30–100 *pairs*\n",
    "\n",
    "**Golden rule:** 20 excellent images > 200 mediocre images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c246c198",
   "metadata": {},
   "source": [
    "### Example: training pairs (reference → target)\n",
    "\n",
    "These pairs show the mapping the model is expected to learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1cfb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATA_DIR = Path('training_data')\n",
    "refs = DATA_DIR / 'pimpmyride_control'\n",
    "tgts = DATA_DIR / 'pimpmyride'\n",
    "\n",
    "pairs = ['001', '003', '017', '037']\n",
    "fig, axes = plt.subplots(2, len(pairs), figsize=(12, 6))\n",
    "for i, idx in enumerate(pairs):\n",
    "    ref_path = refs / f'{idx}.png'\n",
    "    tgt_path = tgts / f'{idx}.png'\n",
    "    if ref_path.exists():\n",
    "        axes[0, i].imshow(Image.open(ref_path))\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title('Reference' if i == 0 else '', fontsize=10)\n",
    "\n",
    "    if tgt_path.exists():\n",
    "        axes[1, i].imshow(Image.open(tgt_path))\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title('Target' if i == 0 else '', fontsize=10)\n",
    "\n",
    "plt.suptitle('Training Pairs: Reference → Target', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a4f29-6b89-4e41-8a78-923ff41f381a",
   "metadata": {},
   "source": [
    "---\n",
    "# Training Results\n",
    "\n",
    "## The Task: Pimp My Ride\n",
    "\n",
    "We trained Klein Base to transform regular cars into custom \"pimped\" versions.\n",
    "\n",
    "**Training Config:**\n",
    "```yaml\n",
    "model: FLUX.2-Klein-Base-4B\n",
    "rank: 32, alpha: 32\n",
    "lr: 1e-4\n",
    "steps: 500\n",
    "dataset: 44 image pairs\n",
    "```\n",
    "\n",
    "## The Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facc4489-c15a-406c-b36e-3ebe89b86d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "pairs = ['001', '003', '017', '037']\n",
    "\n",
    "for i, idx in enumerate(pairs):\n",
    "    ref_path = DATA_DIR / 'pimpmyride_control' / f'{idx}.png'\n",
    "    if ref_path.exists():\n",
    "        axes[0, i].imshow(Image.open(ref_path))\n",
    "    axes[0, i].axis('off')\n",
    "    axes[0, i].set_title('Reference' if i == 0 else '', fontsize=11, color='white')\n",
    "    \n",
    "    tgt_path = DATA_DIR / 'pimpmyride' / f'{idx}.png'\n",
    "    if tgt_path.exists():\n",
    "        axes[1, i].imshow(Image.open(tgt_path))\n",
    "    axes[1, i].axis('off')\n",
    "    axes[1, i].set_title('Target' if i == 0 else '', fontsize=11, color='white')\n",
    "\n",
    "plt.suptitle('Training Data: Reference → Target Pairs', fontsize=14, y=1.02, color='white')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"Dataset: 44 pairs | Diverse car types | Unique captions per image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421a76cb-ada6-4db5-afc4-40c271463ffa",
   "metadata": {},
   "source": [
    "## Training Progression\n",
    "\n",
    "Here's what the model generated at each checkpoint. Same prompt throughout:\n",
    "\n",
    "`\"pimpmyride, a sports car\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36b0f82-d6ff-404b-83dd-f6d9ad550e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Setup for demo images only\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams.update({'figure.facecolor': '#0f0f0f', 'axes.facecolor': '#0f0f0f'})\n",
    "\n",
    "SAMPLES_DIR = Path('samples')\n",
    "DATA_DIR = Path('training_data')\n",
    "\n",
    "print('Ready')\n",
    "\n",
    "\n",
    "# Training progression\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "\n",
    "step_files = {\n",
    "    0: '1768946967141__000000000_0.jpg',\n",
    "    50: '1768947077803__000000050_0.jpg',\n",
    "    100: '1768947186514__000000100_0.jpg',\n",
    "    200: '1768947404491__000000200_0.jpg',\n",
    "    250: '1768947514157__000000250_0.jpg',\n",
    "    300: '1768947621817__000000300_0.jpg',\n",
    "    350: '1768947732234__000000350_0.jpg',\n",
    "    400: '1768947841399__000000400_0.jpg',\n",
    "    450: '1768947950571__000000450_0.jpg',\n",
    "    500: '1768948054553__000000500_0.jpg',\n",
    "}\n",
    "\n",
    "steps = [0, 50, 100, 200, 250, 300, 350, 400, 450, 500]\n",
    "colors = {'early': '#38bdf8', 'learning': '#22c55e', 'converging': '#eab308'}\n",
    "\n",
    "for i, step in enumerate(steps):\n",
    "    row, col = i // 5, i % 5\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    img_path = SAMPLES_DIR / step_files[step]\n",
    "    if img_path.exists():\n",
    "        ax.imshow(Image.open(img_path))\n",
    "    \n",
    "    color = 'white' if step == 0 else colors['early'] if step <= 100 else colors['learning'] if step <= 300 else colors['converging']\n",
    "    ax.set_title(f'Step {step}', fontsize=11, color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Training Progression: Concept Emergence', fontsize=14, y=1.02, color='white')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d385589f-ab4d-4df2-8d68-cd4300468802",
   "metadata": {},
   "source": [
    "## Loss Curve Analysis\n",
    "\n",
    "What the numbers tell us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d154f67-719e-4908-a831-55d7c7cf4db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curve\n",
    "steps = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "losses = [0.949, 0.769, 0.760, 0.695, 0.277, 0.932, 0.431, 0.155, 0.177, 0.15]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "ax.plot(steps, losses, 'o-', color='#38bdf8', lw=2, markersize=8)\n",
    "ax.fill_between(steps, losses, alpha=0.2, color='#38bdf8')\n",
    "\n",
    "ax.annotate('Initial learning', xy=(100, 0.769), xytext=(120, 0.9),\n",
    "           arrowprops=dict(arrowstyle='->', color='white'), fontsize=9, color='white')\n",
    "ax.annotate('Big drop!', xy=(250, 0.277), xytext=(270, 0.45),\n",
    "           arrowprops=dict(arrowstyle='->', color='#22c55e'), fontsize=10, color='#22c55e')\n",
    "ax.annotate('Normal variance', xy=(300, 0.932), xytext=(320, 0.8),\n",
    "           arrowprops=dict(arrowstyle='->', color='#eab308'), fontsize=9, color='#eab308')\n",
    "ax.annotate('Converged', xy=(450, 0.17), xytext=(420, 0.35),\n",
    "           arrowprops=dict(arrowstyle='->', color='#22c55e'), fontsize=10, color='#22c55e')\n",
    "\n",
    "ax.set_xlabel('Training Steps', color='white')\n",
    "ax.set_ylabel('Loss', color='white')\n",
    "ax.set_title('Training Loss Curve', fontsize=14, color='white')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0, 1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observations:\")\n",
    "print(\"  • Step 0-100: Model picking up basic concept\")\n",
    "print(\"  • Step 200-250: Major breakthrough — concept locks in\")\n",
    "print(\"  • Step 300 spike: Normal with batch=1 (variance)\")\n",
    "print(\"  • Step 400+: Converged — could stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa34f8",
   "metadata": {},
   "source": [
    "## Data mistakes that cause overfitting\n",
    "\n",
    "- Near‑duplicates (same pose/angle)\n",
    "- Consistent background colors\n",
    "- Reused camera angles\n",
    "- Watermarks or UI elements\n",
    "\n",
    "**Symptom:** the LoRA learns the dataset instead of the concept.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7977cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Failure modes / troubleshooting\n",
    "display(SVG(ASSETS / 'troubleshooting.svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a9f3b0",
   "metadata": {},
   "source": [
    "---\n",
    "# 6) Practical recipe\n",
    "\n",
    "**One reliable recipe**\n",
    "- **Model**: FLUX.2 [klein] Base 4B\n",
    "- **Rank/Alpha**: 16–32, alpha = rank\n",
    "- **LR**: 1e‑4 to 3e‑4\n",
    "- **Steps**: 500–1200 (watch for overfit)\n",
    "- **Targets**: attention layers attention across all blocks\n",
    "- **Data**: 20–100 *great* images, unique trigger, diverse contexts\n",
    "\n",
    "**Stop training if**\n",
    "- Concept appears without trigger\n",
    "- Prompt changes stop affecting output\n",
    "- Validation/sample quality peaks then degrades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a00d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG, display\n",
    "from pathlib import Path\n",
    "\n",
    "display(SVG(Path('assets') / 'decision_tree.svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84f60c1",
   "metadata": {},
   "source": [
    "6.2 Minimal training loop\n",
    "\n",
    "1. Curate dataset + captions\n",
    "2. Choose targets (attention layers)\n",
    "3. Train 500–1200 steps\n",
    "4. Evaluate prompt suite every 100–200 steps\n",
    "5. Stop when quality peaks\n",
    "\n",
    "**Key idea:** sampling during training beats watching loss alone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538af79a",
   "metadata": {},
   "source": [
    "---\n",
    "# Closing \n",
    "\n",
    "1. **Klein Base (non‑distilled) is the training target**.\n",
    "2. **Where you place LoRA matters** (double‑stream vs single‑stream).\n",
    "3. **Rank/steps/LR** are the primary levers.\n",
    "4. **Data quality** is the biggest driver of success.\n",
    "5. **Validate with prompts, not just loss.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
